{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab as gl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "others \t   The new employee stood before the paper shredder looking confused.     \"Need some help? \" a secretary asked.     \"Yes, \" he replied.  \"How does this thing work? \"     \"Simple, \" she said, taking the fat report from his hand and feeding it into the shredder.     \"Thanks, but where do the copies come out? \"  \n",
      "\n",
      "others \t  America:    8:00 - Welcome to work!  12:00 - Lunch break  17:00 - The work day is over    Japan:    8:00 - Are you already at work?  12:00 - Continue your work  17:00 - The work day is over  20:00 - Please finish your work    Romania:    8:00 - Has anyone come to work?  12:00 - Did someone start working?  16:00 - Is anyone at work?    \n",
      "\n",
      "others \t  An artist asked the gallery owner if there had been any interest in his paintings currently on display.  \"I've got good news and bad news, \" the owner replied.  \"The good news is that a gentleman inquired about your work and wondered if it would appreciate in value after your death. When I told him it would, he bought all fifteen of your paintings. \"     \"That's wonderful! \" the artist exclaimed.  \"What's the bad news? \"    With concern, the gallery owner replied: \"The guy was your doctor. \"    \n",
      "\n",
      "others \tWhy are there so many Jones's in the phone book? Because they all have phones.\n",
      "\n",
      "relationship \t  When my three-year-old son opened the birthday gift from his grandmother, he discovered a water pistol. He squealed with delight and headed for the nearest sink. I was not so pleased. I turned to Mom and said,  \"I'm surprised at you. Don't you remember how we used to drive you crazy with water guns? \"    Mom smiled and then replied... \"I remember. \"    \n",
      "\n",
      "others \tTwo cannibals are eating a clown, one turns to other and says: \"Does this taste funny to you?\n",
      "\n",
      "others \tA lawyer opened the door of his BMW, when suddenly a car came along and hit the door, ripping it off completely. When the police arrived at the scene, the lawyer was complaining bitterly about the damage to his precious BMW. \"Officer, look what they've done to my Beeeeemer!!!\", he whined.  \"You lawyers are so materialistic, you make me sick!!!\" retorted the officer. \"You're so worried about your stupid BMW, that you didn't even notice that your left arm was ripped off!!!\"  \"Oh my gaaaad...\", replied the lawyer, finally noticing the bloody left shoulder where his arm once was. \"Where's my Rolex???!!!!\"\n",
      "\n",
      "others \tQ. What is orange and sounds like a parrot?   A. A carrot.\n",
      "\n",
      "politics \tPresident Clinton looks up from his desk in the Oval Office to see one of his aides nervously approach him.   \"What is it?\" exclaims the President.  \"It's this Abortion Bill Mr. President, what do you want to do about it?\" the aide replies.   \"Just go ahead and pay it.\" responds the President.\n",
      "\n",
      "others \tWhat is the rallying cry of the International Dyslexic Pride movement? Dyslexics Untie!\n",
      "\n",
      "others \t  Judy was having trouble with her computer, so she called Tony, the computer guy, over to her desk. Tony clicked a couple buttons and solved the problem. As he was walking away, Judy called after him,  \"So, what was wrong? \"    And he replied,  \"It was an ID Ten T Error. \"    A puzzled expression ran riot over Judy's face.  \"An ID Ten T Error? What's that...in case I need to fix it again? \"    He gave her a grin... \"Haven't you ever heard of an ID Ten T Error before? \"     \"No, \" replied Judy.     \"Write it down, \" he said,  \"and I think you'll figure it out. \"    (She wrote...) I D 1 0 T  \n",
      "\n",
      "others \tAn engineer, a physicist and a mathematician are sleeping in a room. There is a fire in the room. The engineer wakes up, sees the fire, picks up the bucket of water and douses the fire and goes back to sleep. Again there is fire in the room. This time, the physicist wakes up, notices the bucket, fills it with water, calculates the optimal trajectory and douses the fire in minimum amount of water and goes back to sleep.  Again there is fire. This time the mathematician wakes up. He looks at the fire, looks at the bucket and the water and exclaims, \"A solution exists\" and goes back to sleep.\n",
      "\n",
      "others \tQ: How many Presidents does it take to screw in a light bulb? A: It depends upon your definition of screwing a light bulb.\n",
      "\n",
      "others \t   Sherlock Holmes and Dr. Watson go on a camping trip, set up their tent, and fall asleep. Some hours later, Holmes wakes his faithful friend.  \"Watson, look up at the sky and tell me what you see. \"     Watson replies,  \"I see millions of stars. \"      \"What does that tell you? \"     Watson ponders for a minute.  \"Astronomically speaking, it tells me that there are millions of galaxies and potentially billions of planets. Astrologically, it tells me that Saturn is in Leo. Timewise, it appears to be approximately a quarter past three. Theologically, it's evident the Lord is all-powerful and we are small and insignificant. Meteorologically, it seems we will have a beautiful day tomorrow. What does it tell you? \"     Holmes is silent for a moment, then speaks.  \"Watson, you idiot, someone has stolen our tent. \"    \n",
      "\n",
      "others \t   Chuck Norris' calendar goes straight from March 31st to April 2nd; no one fools Chuck Norris.    \n",
      "\n",
      "0.533333333333\n"
     ]
    }
   ],
   "source": [
    "def extract_key_words(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return continuous_chunk\n",
    "    \n",
    "\n",
    "def clean_joke(joke):\n",
    "    joke = re.sub(r'([^\\.\\s\\w]|_)+', '', joke).replace(\".\", \". \")\n",
    "    joke = joke.replace('\\r', '') \n",
    "    joke = joke.replace('\\n', '')\n",
    "    joke = joke.replace('<br />', '')\n",
    "    joke = joke.replace('<p>', '')\n",
    "    joke = joke.replace('&quot;', '')\n",
    "    joke = joke.replace('&#039;', '')\n",
    "    joke = \" \".join(extract_key_words(joke))\n",
    "    #print joke\n",
    "    return joke\n",
    "\n",
    "def load_joke_classes_and_text():\n",
    "    data = pd.read_csv(\"../data/Jokes_labelling.txt\", delimiter=\"\\t\")\n",
    "    data['Jokes'] = data['Jokes'].map(lambda j: clean_joke(j))\n",
    "    data.drop('joke_category', axis=1, inplace=True)\n",
    "    cat_feats = pd.get_dummies(data['joke_category_reduced'], prefix='cat')\n",
    "    data = pd.concat([data['joke_id'], data['Jokes'], cat_feats], axis=1)\n",
    "    \n",
    "    Y = cat_feats\n",
    "    X = data['Jokes']\n",
    "    #print Y.describe()\n",
    "    #print X.describe()\n",
    "    \n",
    "def load_joke_classes_text_and_glove_vectors():\n",
    "    id_vectors = pd.read_csv(\"../data/Jokes_id_with_vectors.txt\", delimiter=\"\\t\")\n",
    "    id_vectors.drop(\"Unnamed: 301\", axis=1, inplace=True)\n",
    "    data = pd.read_csv(\"../data/Jokes_labelling.txt\", delimiter=\"\\t\")\n",
    "    cat_feats = pd.get_dummies(data['joke_category_reduced'], prefix='cat')\n",
    "    \n",
    "    Y = cat_feats\n",
    "    X = id_vectors\n",
    "    X['jokes'] = data['Jokes']\n",
    "    #X = id_vectors.drop('joke_id', axis=1)\n",
    "    cols = X.columns.tolist()\n",
    "    cols.insert(1, cols.pop(cols.index('jokes')))\n",
    "    X = X.reindex(columns= cols)\n",
    "    \n",
    "    Y = data['joke_category_reduced']\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def evaluting_and_predicting_joke_category_by_words(X,Y):\n",
    "    Y = Y.values\n",
    "    X = X.values\n",
    "    \n",
    "    le = preprocessing.LabelEncoder()\n",
    "    Y_c = le.fit_transform(Y)\n",
    "    \n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y_c, test_size=0.10, random_state=11)\n",
    "    \n",
    "    train_ids = X_train[:, 0]\n",
    "    train_jokes = X_train[:, 1]\n",
    "    test_ids = X_test[:, 0]\n",
    "    test_jokes = X_test[:, 1]\n",
    "    \n",
    "    X_train = X_train[:, 2:]\n",
    "    X_test = X_test[:, 2:]\n",
    "    \n",
    "    model = xgb.XGBClassifier()\n",
    "    model = GaussianNB()\n",
    "    #model = svm.SVC()\n",
    "    #model = LogisticRegression()\n",
    "    #model = NearestCentroid()\n",
    "    #model = KNeighborsClassifier()\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    pred_categories = le.inverse_transform(preds)\n",
    "    #print pred_categories\n",
    "    for id, categ, joke in zip(test_ids, pred_categories, test_jokes):\n",
    "        print categ, \"\\t\", joke\n",
    "        print\n",
    "    \n",
    "    print accuracy_score(y_test, preds)\n",
    "    \n",
    "\n",
    "X, Y = load_joke_classes_text_and_glove_vectors()\n",
    "evaluting_and_predicting_joke_category_by_words(X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
